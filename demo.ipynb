{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "912cd8c6-d405-4dfe-8897-46108e6a6af7",
   "metadata": {},
   "source": [
    "# RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631b09a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: An OpenAI API key must be set here for application initialization, even if not in use.\n",
    "# If you're not utilizing OpenAI models, assign a placeholder string (e.g., \"not_used\").\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-openai-key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf91974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url=\"https://www.cs.umd.edu/class/spring2022/cmsc430/\"\n",
    "r = requests.get(url)\n",
    "\n",
    "root=BeautifulSoup(r.content,\"lxml\")\n",
    "lnks = root.findAll(\"a\")\n",
    "\n",
    "\n",
    "import os\n",
    "from urllib.parse import urljoin\n",
    "os.system('mkdir lectures')\n",
    "outdir = 'lectures/'\n",
    "for lnk in lnks:\n",
    "    href=lnk['href']\n",
    "    if href.lower().endswith(('.pdf','.py')):\n",
    "        urld = urljoin(url, href)\n",
    "        rd = requests.get(urld, stream=True)\n",
    "        #Write the downloaded pdf to a file\n",
    "        filename = urld.split(\"/\")[-1]\n",
    "        print(filename)\n",
    "        outfile = os.path.join(outdir,filename)\n",
    "        with open(outfile,'wb') as f:\n",
    "            f.write(rd.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b84facd8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'href'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m outdir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml_files/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lnk \u001b[38;5;129;01min\u001b[39;00m lnks:\n\u001b[0;32m---> 16\u001b[0m     href \u001b[38;5;241m=\u001b[39m \u001b[43mlnk\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhref\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m href\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.html\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m href\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.htm\u001b[39m\u001b[38;5;124m'\u001b[39m):  \u001b[38;5;66;03m# Filter for HTML\u001b[39;00m\n\u001b[1;32m     18\u001b[0m         urld \u001b[38;5;241m=\u001b[39m urljoin(url, href)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/bs4/element.py:1573\u001b[0m, in \u001b[0;36mTag.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1570\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m   1571\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"tag[key] returns the value of the 'key' attribute for the Tag,\u001b[39;00m\n\u001b[1;32m   1572\u001b[0m \u001b[38;5;124;03m    and throws an exception if it's not there.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1573\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'href'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "url = \"https://www.cs.umd.edu/class/spring2022/cmsc430/Notes.html\"\n",
    "r = requests.get(url)\n",
    "\n",
    "root = BeautifulSoup(r.content, \"lxml\")\n",
    "lnks = root.findAll(\"a\")\n",
    "\n",
    "os.makedirs('html_files', exist_ok=True)  # Create the output directory\n",
    "outdir = 'html_files/'\n",
    "\n",
    "for lnk in lnks:\n",
    "    href = lnk['href']\n",
    "    if href.lower().endswith('.html') or href.lower().endswith('.htm'):  # Filter for HTML\n",
    "        urld = urljoin(url, href)\n",
    "        rd = requests.get(urld)\n",
    "\n",
    "        # Write the downloaded HTML to a file\n",
    "        filename = urld.split(\"/\")[-1]\n",
    "        if not filename:  # Handle cases where the filename is empty\n",
    "            filename = 'index.html'  # Default filename\n",
    "        outfile = os.path.join(outdir, filename)\n",
    "\n",
    "        with open(outfile, 'w', encoding='utf-8') as f:  # Note the 'w' for text mode\n",
    "            f.write(rd.text)  # .text gets the HTML content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a207d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def convert_html_to_text(input_dir, output_dir):\n",
    "    \"\"\"Converts HTML files in a directory to text files.\n",
    "\n",
    "    Args:\n",
    "        input_dir (str): Path to the directory containing HTML files.\n",
    "        output_dir (str): Path to the directory where text files will be saved.\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Create the output directory\n",
    "\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith('.html') or filename.endswith('.htm'):\n",
    "            filepath = os.path.join(input_dir, filename)\n",
    "            with open(filepath, 'r',encoding='utf-8') as html_file:\n",
    "                soup = BeautifulSoup(html_file, 'html.parser')\n",
    "\n",
    "                # Get the text content\n",
    "                text = soup.get_text(strip=True)\n",
    "\n",
    "                # Create output filename (replace .html with .txt)\n",
    "                output_filename = os.path.splitext(filename)[0] + '.txt'\n",
    "                output_filepath = os.path.join(output_dir, output_filename)\n",
    "\n",
    "                with open(output_filepath, 'w', encoding='utf-8') as text_file:\n",
    "                    text_file.write(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9ecd024",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def append_text_files(input_dir, output_file):\n",
    "\toutdir = os.path.join(input_dir, output_file)\n",
    "\tprint(outdir)\n",
    "\twith open(outdir, 'w', encoding='utf-8') as outfile:\n",
    "\t\tfor filename in os.listdir(input_dir):\n",
    "\t\t\tif filename.endswith('.txt'):\n",
    "\t\t\t\tfilepath = os.path.join(input_dir, filename)\n",
    "\t\t\t\twith open(filepath, 'r', encoding='utf-8') as infile:\n",
    "\t\t\t\t\toutfile.write(infile.read())\n",
    "\t\t\t\t\toutfile.write('\\n')  # Add a newline between files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e7d8c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_html_to_text(\"html_files\", \"text_files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a132b672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_files/notes.txt\n"
     ]
    }
   ],
   "source": [
    "append_text_files(\"text_files\", \"notes.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2d7d995-7beb-40b5-9a44-afd350b7d221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 Loot: lambda the ultimate►CMSC 430:Design and Implementation of Programming LanguagesSyllabusText\n"
     ]
    }
   ],
   "source": [
    "# Cinderella story defined in sample.txt\n",
    "with open('text_files/Loot.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d51ebd-5597-4fdd-8c37-32636395081b",
   "metadata": {},
   "source": [
    "1) **Building**: RAPTOR recursively embeds, clusters, and summarizes chunks of text to construct a tree with varying levels of summarization from the bottom up. You can create a tree from the text in 'sample.txt' using `RA.add_documents(text)`.\n",
    "\n",
    "2) **Querying**: At inference time, the RAPTOR model retrieves information from this tree, integrating data across lengthy documents at different abstraction levels. You can perform queries on the tree with `RA.answer_question`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f58830-9004-48a4-b50e-61a855511d24",
   "metadata": {},
   "source": [
    "### Building the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3753fcf9-0a8e-4ab3-bf3a-6be38ef6cd1e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'raptor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mraptor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RetrievalAugmentation \n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'raptor'"
     ]
    }
   ],
   "source": [
    "from raptor import RetrievalAugmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e843edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "RA = RetrievalAugmentation()\n",
    "\n",
    "# construct the tree\n",
    "RA.add_documents(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f219d60a-1f0b-4cee-89eb-2ae026f13e63",
   "metadata": {},
   "source": [
    "### Querying from the tree\n",
    "\n",
    "```python\n",
    "question = # any question\n",
    "RA.answer_question(question)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4037c5-ad5a-424b-80e4-a67b8e00773b",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How did Cinderella reach her happy ending ?\"\n",
    "\n",
    "answer = RA.answer_question(question=question)\n",
    "\n",
    "print(\"Answer: \", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5be7e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tree by calling RA.save(\"path/to/save\")\n",
    "SAVE_PATH = \"demo/cinderella\"\n",
    "RA.save(SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e845de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load back the tree by passing it into RetrievalAugmentation\n",
    "\n",
    "RA = RetrievalAugmentation(tree=SAVE_PATH)\n",
    "\n",
    "answer = RA.answer_question(question=question)\n",
    "print(\"Answer: \", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277ab6ea-1c79-4ed1-97de-1c2e39d6db2e",
   "metadata": {},
   "source": [
    "## Using other Open Source Models for Summarization/QA/Embeddings\n",
    "\n",
    "If you want to use other models such as Llama or Mistral, you can very easily define your own models and use them with RAPTOR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86cbe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from raptor import BaseSummarizationModel, BaseQAModel, BaseEmbeddingModel, RetrievalAugmentationConfig\n",
    "from transformers import AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5cef43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to use the Gemma, you will need to authenticate with HuggingFace, Skip this step, if you have the model already downloaded\n",
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245b91a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "import torch\n",
    "\n",
    "# You can define your own Summarization model by extending the base Summarization Class. \n",
    "class GEMMASummarizationModel(BaseSummarizationModel):\n",
    "    def __init__(self, model_name=\"google/gemma-2b-it\"):\n",
    "        # Initialize the tokenizer and the pipeline for the GEMMA model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.summarization_pipeline = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model_name,\n",
    "            model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "            device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),  # Use \"cpu\" if CUDA is not available\n",
    "        )\n",
    "\n",
    "    def summarize(self, context, max_tokens=150):\n",
    "        # Format the prompt for summarization\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": f\"Write a summary of the following, including as many key details as possible: {context}:\"}\n",
    "        ]\n",
    "        \n",
    "        prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        \n",
    "        # Generate the summary using the pipeline\n",
    "        outputs = self.summarization_pipeline(\n",
    "            prompt,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            top_p=0.95\n",
    "        )\n",
    "        \n",
    "        # Extracting and returning the generated summary\n",
    "        summary = outputs[0][\"generated_text\"].strip()\n",
    "        return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a171496d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GEMMAQAModel(BaseQAModel):\n",
    "    def __init__(self, model_name= \"google/gemma-2b-it\"):\n",
    "        # Initialize the tokenizer and the pipeline for the model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.qa_pipeline = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model_name,\n",
    "            model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "            device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "        )\n",
    "\n",
    "    def answer_question(self, context, question):\n",
    "        # Apply the chat template for the context and question\n",
    "        messages=[\n",
    "              {\"role\": \"user\", \"content\": f\"Given Context: {context} Give the best full answer amongst the option to question {question}\"}\n",
    "        ]\n",
    "        prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        \n",
    "        # Generate the answer using the pipeline\n",
    "        outputs = self.qa_pipeline(\n",
    "            prompt,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            top_p=0.95\n",
    "        )\n",
    "        \n",
    "        # Extracting and returning the generated answer\n",
    "        answer = outputs[0][\"generated_text\"][len(prompt):]\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878f7c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "class SBertEmbeddingModel(BaseEmbeddingModel):\n",
    "    def __init__(self, model_name=\"sentence-transformers/multi-qa-mpnet-base-cos-v1\"):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def create_embedding(self, text):\n",
    "        return self.model.encode(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255791ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAC = RetrievalAugmentationConfig(summarization_model=GEMMASummarizationModel(), qa_model=GEMMAQAModel(), embedding_model=SBertEmbeddingModel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee46f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "RA = RetrievalAugmentation(config=RAC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe05daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('demo/sample.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "    \n",
    "RA.add_documents(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eee5847",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How did Cinderella reach her happy ending?\"\n",
    "\n",
    "answer = RA.answer_question(question=question)\n",
    "\n",
    "print(\"Answer: \", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
